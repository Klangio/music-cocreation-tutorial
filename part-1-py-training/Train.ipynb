{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Music Co-creation Tutorial Part 1 (Training).ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK41YT0yH3vs"
      },
      "source": [
        "# Music Co-creation Tutorial Part 1: Training a generative model of music\n",
        "### [Chris Donahue](https://chrisdonahue.com), [Anna Huang](https://research.google/people/105787/), [Jon Gillick](https://www.jongillick.com/)\n",
        "\n",
        "This is the first part of a two-part tutorial entitled [*Interactive music co-creation with PyTorch and TensorFlow.js*](https://github.com/chrisdonahue/music-cocreation-tutorial/), prepared as part of the ISMIR 2021 tutorial *Designing generative models for interactive co-creation*. This part of the tutorial will demonstrate how to **train a generative model of music in PyTorch**, and **port its weights to TensorFlow.js** format for interaction. [See our GitHub repo](https://github.com/chrisdonahue/music-cocreation-tutorial/) for part 2.\n",
        "\n",
        "## Primer on Piano Genie\n",
        "\n",
        "The generative model we will train is called [Piano Genie](https://magenta.tensorflow.org/pianogenie) (Donahue et al. 2019). Piano Genie is a system which maps amateur improvisations on a miniature 8-button keyboard ([video](https://www.youtube.com/watch?v=YRb0XAnUpIk), [demo](https://piano-genie.glitch.me)) into realistic performances on a full 88-key piano.\n",
        "\n",
        "To achieve this, Piano Genie adopts an _autoencoder_ approach. First, an _encoder_ maps professional piano performances into this 8-button space. Then, a _decoder_ attempts to reconstruct the original piano performance from the 8-button version. The entire system is trained end-to-end to minimize the decoder's reconstruction error. At performance time, we replace the encoder with a user improvising on an 8-button controller, and use the pre-trained decoder to generate a corresponding piano performance.\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/pmYajEg.png\" width=600px/></center>\n",
        "\n",
        "At a low-level, both the encoder and the decoder for Piano Genie are lightweight recurrent neural networks, which are suitable for real-time performance even on mobile CPUs. The discrete bottleneck is achieved using a technique called _integer-quantized autoencoding_ (IQAE), which was also proposed in the Piano Genie paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "an946C19rSVJ"
      },
      "source": [
        "#@title **(Step 1)** Parse MIDI piano performances into simple lists of notes\n",
        "\n",
        "USE_PRECACHED = True  # @param{type:\"boolean\"}\n",
        "\n",
        "# @markdown To train Piano Genie, we will use a dataset of professional piano performances called [MAESTRO](https://magenta.tensorflow.org/datasets/maestro) (Hawthorne et al. 2019).\n",
        "# @markdown Each performance in this dataset was captured by a Disklavier, a computerized piano which can record human performances in MIDI format, i.e., as timestamped sequences of notes.\n",
        "\n",
        "PIANO_LOWEST_KEY_MIDI_PITCH = 21\n",
        "PIANO_NUM_KEYS = 88\n",
        "\n",
        "import gzip\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "def download_and_parse_maestro():\n",
        "    # Install pretty_midi\n",
        "    !!pip install pretty_midi\n",
        "    import pretty_midi\n",
        "\n",
        "    # Download MAESTRO dataset (Hawthorne+ 2018)\n",
        "    !!wget -nc https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip\n",
        "    !!unzip maestro-v2.0.0-midi.zip\n",
        "\n",
        "    # Parse MAESTRO dataset\n",
        "    dataset = defaultdict(list)\n",
        "    with open(\"maestro-v2.0.0/maestro-v2.0.0.json\", \"r\") as f:\n",
        "        for attrs in tqdm(json.load(f)):\n",
        "            split = attrs[\"split\"]\n",
        "            midi = pretty_midi.PrettyMIDI(\"maestro-v2.0.0/\" + attrs[\"midi_filename\"])\n",
        "            assert len(midi.instruments) == 1\n",
        "            # @markdown Formally, a piano performance is a sequence of notes: $\\mathbf{x} = (x_1, \\ldots, x_N)$, where each $x_i = (t_i, x^e_i, k_i, x^v_i)$, signifying:\n",
        "            notes = [\n",
        "                (\n",
        "                    # @markdown 1. (When the key was pressed) An _onset_ time $t_i \\in \\mathbb{T}$, where $\\mathbb{T} = \\{ t \\in \\mathbb{R} \\mid 0 \\leq t \\leq T \\}$ \n",
        "                    float(n.start),\n",
        "                    # @markdown 2. (When the key was released) An _offset_ time $x^e_i \\in \\mathbb{T}$\n",
        "                    float(n.end),\n",
        "                    # @markdown 3. (Which key was pressed) A _key_ index $k_i \\in \\mathbb{K}$, where $\\mathbb{K} = \\{\\text{A0}, \\ldots, \\text{C8}\\}$ and $|\\mathbb{K}| = 88$\n",
        "                    int(n.pitch - PIANO_LOWEST_KEY_MIDI_PITCH),\n",
        "                    # @markdown 4. (How hard the key was pressed) A _velocity_ $x^v_i \\in \\mathbb{V}$, where $\\mathbb{V} = \\{1, \\ldots, 127\\}$\n",
        "                    int(n.velocity),\n",
        "                )\n",
        "                for n in midi.instruments[0].notes\n",
        "            ]\n",
        "\n",
        "            # This list is in sorted order of onset time, i.e., $x_{i-1}^s \\leq x_i^s ~\\forall~i \\in \\{2, \\ldots, N\\}$.\n",
        "            notes = sorted(notes, key=lambda n: (n[0], n[2]))\n",
        "            assert all(\n",
        "                [\n",
        "                    all(\n",
        "                        [\n",
        "                            # Start times should be non-negative\n",
        "                            n[0] >= 0,\n",
        "                            # Note durations should be strictly positive, i.e., $x_i^s < x_i^e$\n",
        "                            n[0] < n[1],\n",
        "                            # Key index should be in range of the piano\n",
        "                            0 <= n[2] and n[2] < PIANO_NUM_KEYS,\n",
        "                            # Velocity should be valid\n",
        "                            1 <= n[3] and n[3] < 128,\n",
        "                        ]\n",
        "                    )\n",
        "                    for n in notes\n",
        "                ]\n",
        "            )\n",
        "            dataset[split].append(notes)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "\n",
        "if USE_PRECACHED:\n",
        "    !!wget -nc https://github.com/chrisdonahue/music-cocreation-tutorial/raw/main/part-1-py-training/data/maestro-v2.0.0-simple.json.gz\n",
        "    with gzip.open(\"maestro-v2.0.0-simple.json.gz\", \"rb\") as f:\n",
        "        DATASET = json.load(f)\n",
        "else:\n",
        "    DATASET = download_and_parse_maestro()\n",
        "    with gzip.open(\"maestro-v2.0.0-simple.json.gz\", \"w\") as f:\n",
        "        f.write(json.dumps(DATASET).encode(\"utf-8\"))\n",
        "\n",
        "print([(s, len(DATASET[s])) for s in [\"train\", \"validation\", \"test\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UQ9PAvMCwd2"
      },
      "source": [
        "# @title **(Step 2)** Define Piano Genie autoencoder\n",
        "\n",
        "# @markdown Our intended interaction for Piano Genie is to have users perform on a miniature 8-button keyboard.\n",
        "# @markdown Similarly to how we formalized professional piano performances, we will represent these \"button performances\" as sequences of \"notes\", where we replace piano keys $k_i$ with buttons $b_i$, and we remove velocity since our controllers are not velocity-sensitive. So, to summarize, piano performances $\\mathbf{x}$ and button performances $\\mathbf{c}$ are defined as follows:\n",
        "\n",
        "# @markdown - $\\mathbf{x} = (x_1, \\ldots, x_N)$, where $x_i = (t_i \\in \\mathbb{T}, x^e_i \\in \\mathbb{T}, k_i \\in \\mathbb{K}, x^v_i \\in \\mathbb{V})$, i.e., (onsets, offsets, keys, velocities)\n",
        "\n",
        "# @markdown - $\\mathbf{c} = (c_1, \\ldots, c_M)$, where $c_i = (c^s_i \\in \\mathbb{T}, c^e_i \\in \\mathbb{T}, b_i \\in \\mathbb{B})$, i.e., (onsets, offsets, buttons), and $\\mathbb{B} = \\{ \\color{#EE2B29}\\blacksquare, \\color{#ff9800}\\blacksquare, \\color{#ffff00}\\blacksquare, \\color{#c6ff00}\\blacksquare, \\color{#00e5ff}\\blacksquare, \\color{#2979ff}\\blacksquare, \\color{#651fff}\\blacksquare, \\color{#d500f9}\\blacksquare \\}$\n",
        "\n",
        "# @markdown To map button performances into piano performances, we will train a generative model $P(\\mathbf{x} \\mid \\mathbf{c})$.\n",
        "# @markdown In practice, we will factorize this joint distribution over note sequences $\\mathbf{x}$ into the product of conditional probabilities of individual notes: $P(\\mathbf{x} \\mid \\mathbf{c}) = \\prod_{i=1}^{N} P(x_i \\mid \\mathbf{x}_{< i}, \\mathbf{c})$. \n",
        "\n",
        "# @markdown Hence, our **overall goal is to learn** $P(x_i \\mid \\mathbf{x}_{< i}, \\mathbf{c})$, \n",
        "# @markdown which we will **approximate by modeling**:\n",
        "\n",
        "# @markdown <center>$P(k_i \\mid \\mathbf{k}_{<i}, \\mathbf{t}_{\\leq i}, \\mathbf{b}_{\\leq i})$.</center>\n",
        "\n",
        "# @markdown We arrived at this approximation by working through constraints imposed by the interaction (details at the end)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_vnVHmw1VB5",
        "cellView": "form"
      },
      "source": [
        "# @markdown #### Decoder\n",
        "\n",
        "# @markdown <center><img src=\"https://i.imgur.com/phEiaJZ.png\" width=600px/></center>\n",
        "\n",
        "# @markdown The approximation $P(k_i \\mid \\mathbf{k}_{<i}, \\mathbf{t}_{\\leq i}, \\mathbf{b}_{\\leq i})$ constitutes the decoder of Piano Genie, which we will parameterize using an RNN.\n",
        "# @markdown To achieve our real-time interaction, we will compute and sample from this RNN at the instant the user presses a button, passing as input the key from the previous timestep, the current time, the button the user pressed, and a vector which summarizes the ongoing history.\n",
        "\n",
        "At each timestep, the RNN receives the key from the previous timestep, the current onset time, and the \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "SOS = PIANO_NUM_KEYS\n",
        "\n",
        "class PianoGenieDecoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        delta_time_max=1.,\n",
        "        rnn_dim=128,\n",
        "        rnn_num_layers=2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.delta_time_max = delta_time_max\n",
        "        self.rnn_dim = rnn_dim\n",
        "        self.rnn_num_layers = rnn_num_layers\n",
        "        self.input = nn.Linear(PIANO_NUM_KEYS + 3, rnn_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            rnn_dim,\n",
        "            rnn_dim,\n",
        "            rnn_num_layers,\n",
        "            bias=True,\n",
        "            batch_first=True,\n",
        "            bidirectional=False,\n",
        "        )\n",
        "        self.output = nn.Linear(rnn_dim, 88)\n",
        "\n",
        "    # @markdown Formally, the decoder is a function:\n",
        "    # @markdown $D_{\\theta}: k_{i-1}, t_i, b_i, \\mathbf{h}_{i-1} \\mapsto \\mathbf{\\hat{k}}_i, \\mathbf{h}_i$, where:\n",
        "    def forward(self, k_i, t_i, b_i, h_im1=None):\n",
        "        # Convert time into delta time for stability\n",
        "        dt = torch.diff(t_i, dim=1)\n",
        "        dt_i = torch.cat([torch.full_like(dt[:, :1], 1e6), dt], dim=1)\n",
        "        dt_i = torch.minimum(dt_i, self.delta_time_max)\n",
        "\n",
        "        # @markdown - $\\mathbf{h}_i$ is a vector summarizing timesteps $1, \\ldots, i$\n",
        "\n",
        "        # @markdown - $\\mathbf{h}_0$ is some initial value (zeros) for that vector\n",
        "        if h_im1 is None:\n",
        "            # NOTE: PyTorch uses zeros automatically if h is None\n",
        "            pass\n",
        "\n",
        "        # @markdown - $k_0$ is a special start-of-sequence token $<\\text{S}>$\n",
        "        k_im1 = torch.cat([torch.full_like(k_i[:, :1], SOS), k_i[:, :-1]], dim=1)\n",
        "\n",
        "        inputs = [\n",
        "            # k_im1\n",
        "            F.one_hot(k_im1, PIANO_NUM_KEYS + 1),\n",
        "            # t_i\n",
        "            dt_i.unsqueeze(dim=2),\n",
        "            # b_i\n",
        "            b_i.unsqueeze(dim=2),\n",
        "        ]\n",
        "        x = self.input(torch.cat(inputs, dim=2))\n",
        "        x, h_i = self.lstm(x, h_im1)\n",
        "        # @markdown - $\\mathbf{\\hat{k}}_i \\in \\mathbb{R}^{88}$ are the output logits for timestep $i$\n",
        "        k_hat_i = self.output(x)\n",
        "\n",
        "        return k_hat_i, h_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjEfxkIUQ7TG"
      },
      "source": [
        "# @markdown #### Encoder\n",
        "\n",
        "# @markdown Because we lack examples of human button performances, we use an encoder to automatically learn to map piano performances into synthetic button performances.\n",
        "# @markdown Our encoder is also an RNN, though it is bidirectional unlike the decoder. \n",
        "# @markdown This allows it to observe the entire piano performance before compressing it into buttons.\n",
        "\n",
        "# @markdown Formally, the encoder is a function: $E_{\\varphi} : k_i, t_i, \\mathbf{h^f}_{i-1}, \\mathbf{h^b}_{i-1} \\mapsto b_i$, where $\\mathbf{h^f}_i$ and $\\mathbf{h^b}_i$ are summary vectors in the forwards and backwards directions respectively.\n",
        "\n",
        "class PianoGenieEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        rnn_dim=128,\n",
        "        rnn_num_layers=2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.rnn_dim = rnn_dim\n",
        "        self.rnn_num_layers = rnn_num_layers\n",
        "        self.input = nn.Linear(input_dim, rnn_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            rnn_dim,\n",
        "            rnn_dim,\n",
        "            rnn_num_layers,\n",
        "            bias=True,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "        )\n",
        "        self.output = nn.Linear(rnn_dim * 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        x = self.input(x)\n",
        "        h = (\n",
        "            torch.zeros(\n",
        "                2 * self.rnn_num_layers, batch_size, self.rnn_dim, device=x.device\n",
        "            ),\n",
        "            torch.zeros(\n",
        "                2 * self.rnn_num_layers, batch_size, self.rnn_dim, device=x.device\n",
        "            ),\n",
        "        )\n",
        "        x, h = self.lstm(x, h)\n",
        "        x = self.output(x)\n",
        "        return x[:, :, 0]\n",
        "\n",
        "\n",
        "class IntegerQuantizer(nn.Module):\n",
        "    \"\"\"\n",
        "    IntegerQuantizer independently quantizes scalar values to K values between [-1, 1].\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def real_to_discrete(self, x, eps=1e-6):\n",
        "        x = (x + 1) / 2\n",
        "        x = torch.clamp(x, 0, 1)\n",
        "        x *= self.vocab_size - 1\n",
        "        x = (torch.round(x) + eps).long()\n",
        "        return x\n",
        "\n",
        "    def discrete_to_real(self, x):\n",
        "        x = x.float()\n",
        "        x /= self.vocab_size - 1\n",
        "        x = (x * 2) - 1\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, output_discrete=False):\n",
        "        # Quantize and compute delta (used for straight-through estimator)\n",
        "        with torch.no_grad():\n",
        "            x_disc = self.real_to_discrete(x)\n",
        "            x_quant = self.discrete_to_real(x_disc)\n",
        "            x_quant_delta = x_quant - x\n",
        "\n",
        "        # Quantize w/ straight-through estimator\n",
        "        x = x + x_quant_delta\n",
        "\n",
        "        result = x\n",
        "        if output_discrete:\n",
        "            result = (x, x_disc)\n",
        "        return result\n",
        "\n",
        "\n",
        "class PianoGenieAutoencoder(nn.Module):\n",
        "    \"\"\"\n",
        "    PianoGenieAutoencoder composes encoder, quantizer, decoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.enc = None\n",
        "        if cfg[\"model_enc\"]:\n",
        "            self.enc = PianoGenieEncoder(\n",
        "                # Delta time + key one-hot\n",
        "                1 + PIANO_NUM_KEYS,\n",
        "                rnn_dim=cfg[\"model_rnn_dim\"],\n",
        "                rnn_num_layers=cfg[\"model_rnn_num_layers\"],\n",
        "            )\n",
        "            self.quant = IntegerQuantizer(cfg[\"num_buttons\"])\n",
        "        self.dec = PianoGenieDecoder(\n",
        "            # Delta time + key one-hot + <SOS> + Button\n",
        "            1 + PIANO_NUM_KEYS + 1 + int(cfg[\"model_enc\"]),\n",
        "            PIANO_NUM_KEYS,\n",
        "            rnn_dim=cfg[\"model_rnn_dim\"],\n",
        "            rnn_num_layers=cfg[\"model_rnn_num_layers\"],\n",
        "        )\n",
        "\n",
        "    def forward(self, onset_dts, onset_keys):\n",
        "        if self.enc is None:\n",
        "            enc = None\n",
        "            enc_quant_disc = None\n",
        "        else:\n",
        "            # Run encoder\n",
        "            enc = self.enc(\n",
        "                torch.cat(\n",
        "                    [\n",
        "                        # Feature: onset delta time\n",
        "                        onset_dts.unsqueeze(dim=2),\n",
        "                        # Feature: onset key index\n",
        "                        F.one_hot(onset_keys, PIANO_NUM_KEYS),\n",
        "                    ],\n",
        "                    dim=2,\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Quantize\n",
        "            enc_quant, enc_quant_disc = self.quant(enc, output_discrete=True)\n",
        "\n",
        "        # Run decoder\n",
        "        dec_inputs = [\n",
        "            # Feature: onset delta time\n",
        "            onset_dts.unsqueeze(dim=2),\n",
        "            # Feature: *last* onset key index (prepended with <SOS>)\n",
        "            F.one_hot(\n",
        "                torch.cat(\n",
        "                    [\n",
        "                        torch.full_like(onset_keys[:, :1], PIANO_NUM_KEYS),\n",
        "                        onset_keys[:, :-1],\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                ),\n",
        "                PIANO_NUM_KEYS + 1,\n",
        "            ),\n",
        "        ]\n",
        "        if self.enc is not None:\n",
        "            # Feature: button from encoder\n",
        "            dec_inputs.append(enc_quant.unsqueeze(dim=2))\n",
        "        onset_key_logits, _ = self.dec(torch.cat(dec_inputs, dim=2))\n",
        "\n",
        "        return onset_key_logits, enc, enc_quant_disc\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cBwRkvgTGai"
      },
      "source": [
        "# @markdown #### Approximating $P(x_i \\mid \\mathbf{x}_{< i}, \\mathbf{c})$\n",
        "\n",
        "# @markdown This section walks through how we designed an approximation to $P(x_i \\mid \\mathbf{x}_{< i}, \\mathbf{c})$ which would be appropriate for our intended interaction. You probably don't need to understand this, but some may find it helpful as an illustration of how to design a generative model around constraints imposed by interaction.\n",
        "\n",
        "# @markdown First, we expand the terms:\n",
        "\n",
        "# @markdown <center>$P(x_i \\mid \\mathbf{x}_{< i}, \\mathbf{c}) = P(t_i, x^e_i, k_i, x^v_i \\mid \\mathbf{t}_{<i}, \\mathbf{x^e}_{<i}, \\mathbf{k}_{<i}, \\mathbf{x^v}_{<i}, \\mathbf{c^s}, \\mathbf{c^e}, \\mathbf{b})$</center>\n",
        "\n",
        "# @markdown We think it might be intuitive for the miniature piano to behave like a real piano: pressing a button causes a note to sound, which is held until released. Hence, $N = M$, $t_i = c^s_i$, and $x^e_i = c^e_i$, so we can remove some redundant terms:\n",
        "\n",
        "# @markdown <center>$= P(k_i, x^v_i \\mid \\mathbf{k}_{<i}, \\mathbf{x^v}_{<i}, \\mathbf{t}, \\mathbf{c^e}, \\mathbf{b})$</center>\n",
        "\n",
        "# @markdown Beacuse we want this interaction to be real-time, we must remove any term that might not be available at $t_i$, which includes future onsets $\\mathbf{t}_{>i}$, future buttons $\\mathbf{b}_{>i}$, and all offsets $\\mathbf{c^e}$, since notes can be held indefinitely:\n",
        "\n",
        "# @markdown <center>$\\approx P(k_i, x^v_i \\mid \\mathbf{k}_{<i}, \\mathbf{x^v}_{<i}, \\mathbf{t}_{\\leq i}, \\mathbf{b}_{\\leq i})$</center>\n",
        "\n",
        "# @markdown Finally, we anticipate that it will be frustrating for users if the model predicts dynamics on their behalf, so we remove velocity terms $\\mathbf{x^v}$:\n",
        "\n",
        "# @markdown <center>$\\approx P(k_i, \\mid \\mathbf{k}_{<i}, \\mathbf{t}_{\\leq i}, \\mathbf{b}_{\\leq i})$</center>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_fReq-uCfoy",
        "cellView": "form"
      },
      "source": [
        "# @title **(Step 3)** Train Piano Genie\n",
        "\n",
        "USE_WANDB = False  # @param{type:\"boolean\"}\n",
        "\n",
        "CFG = {\n",
        "    \"seed\": 0,\n",
        "    # Number of buttons in interface\n",
        "    \"num_buttons\": 8,\n",
        "    # Onset delta times will be clipped to this maximum\n",
        "    \"data_delta_time_max\": 1.0,\n",
        "    # Max time stretch for data augmentation (+- 5%)\n",
        "    \"data_augment_time_stretch_max\": 0.05,\n",
        "    # Max transposition for data augmentation (+- tritone)\n",
        "    \"data_augment_transpose_max\": 6,\n",
        "    # Enables encoder\n",
        "    \"model_enc\": True,\n",
        "    # RNN dimensionality\n",
        "    \"model_rnn_dim\": 128,\n",
        "    # RNN num layers\n",
        "    \"model_rnn_num_layers\": 2,\n",
        "    # Training hyperparameters\n",
        "    \"batch_size\": 32,\n",
        "    \"seq_len\": 128,\n",
        "    \"lr\": 3e-4,\n",
        "    \"loss_margin_multiplier\": 1.0,\n",
        "    \"loss_contour_multiplier\": 1.0,\n",
        "    \"summarize_frequency\": 128,\n",
        "    \"eval_frequency\": 128,\n",
        "}\n",
        "\n",
        "import pathlib\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "if USE_WANDB:\n",
        "    try:\n",
        "        import wandb\n",
        "    except ModuleNotFoundError:\n",
        "        !!pip install wandb\n",
        "        import wandb\n",
        "\n",
        "# Init\n",
        "run_dir = pathlib.Path(\"piano_genie\")\n",
        "run_dir.mkdir(exist_ok=True)\n",
        "with open(pathlib.Path(run_dir, \"cfg.json\"), \"w\") as f:\n",
        "    f.write(json.dumps(CFG, indent=2))\n",
        "if USE_WANDB:\n",
        "    wandb.init(project=\"piano-genie\", name=\"tutorial\", config=CFG, reinit=True)\n",
        "\n",
        "# Set seed\n",
        "if CFG[\"seed\"] is not None:\n",
        "    random.seed(CFG[\"seed\"])\n",
        "    np.random.seed(CFG[\"seed\"])\n",
        "    torch.manual_seed(CFG[\"seed\"])\n",
        "    torch.cuda.manual_seed_all(CFG[\"seed\"])\n",
        "\n",
        "# Create models\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = PianoGenieAutoencoder(CFG)\n",
        "model.train()\n",
        "model.to(device)\n",
        "print(\"-\" * 80)\n",
        "for n, p in model.named_parameters():\n",
        "    print(f\"{n}, {p.shape}\")\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=CFG[\"lr\"])\n",
        "\n",
        "# Subsamples performances to create a minibatch\n",
        "def performances_to_batch(performances, device, train=True):\n",
        "    batch_onset_dt = []\n",
        "    batch_onset_keys = []\n",
        "    for p in performances:\n",
        "        # Subsample seq_len notes from performance\n",
        "        assert len(p) >= CFG[\"seq_len\"]\n",
        "        if train:\n",
        "            subsample_offset = random.randrange(0, len(p) - CFG[\"seq_len\"])\n",
        "        else:\n",
        "            subsample_offset = 0\n",
        "        subsample = p[subsample_offset : subsample_offset + CFG[\"seq_len\"]]\n",
        "        assert len(subsample) == CFG[\"seq_len\"]\n",
        "\n",
        "        # Data augmentation\n",
        "        if train:\n",
        "            stretch_factor = (\n",
        "                1\n",
        "                + (random.random() * CFG[\"data_augment_time_stretch_max\"] * 2)\n",
        "                - CFG[\"data_augment_time_stretch_max\"]\n",
        "            )\n",
        "            transposition_factor = random.randint(\n",
        "                -CFG[\"data_augment_transpose_max\"], CFG[\"data_augment_transpose_max\"]\n",
        "            )\n",
        "            subsample = [\n",
        "                (\n",
        "                    n[0] * stretch_factor,\n",
        "                    n[1] * stretch_factor,\n",
        "                    max(0, min(n[2] + transposition_factor, PIANO_NUM_KEYS - 1)),\n",
        "                    n[3],\n",
        "                )\n",
        "                for n in subsample\n",
        "            ]\n",
        "\n",
        "        # Compute onset features\n",
        "        onset_dt = np.diff([n[0] for n in subsample])\n",
        "        onset_dt = np.concatenate([[1e8], onset_dt])\n",
        "        onset_dt = np.clip(onset_dt, 0, CFG[\"data_delta_time_max\"])\n",
        "        batch_onset_dt.append(onset_dt)\n",
        "\n",
        "        # Compute onset keys\n",
        "        batch_onset_keys.append([n[2] for n in subsample])\n",
        "\n",
        "    return (\n",
        "        torch.tensor(np.array(batch_onset_dt)).float(),\n",
        "        torch.tensor(np.array(batch_onset_keys)).long(),\n",
        "    )\n",
        "\n",
        "\n",
        "# Train\n",
        "step = 0\n",
        "best_eval_loss_recons = float(\"inf\")\n",
        "while True:\n",
        "    if step % CFG[\"eval_frequency\"] == 0:\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            eval_metrics = defaultdict(list)\n",
        "            for i in range(0, len(DATASET[\"validation\"]), CFG[\"batch_size\"]):\n",
        "                eval_batch = performances_to_batch(\n",
        "                    DATASET[\"validation\"][i : i + CFG[\"batch_size\"]],\n",
        "                    device,\n",
        "                    train=False,\n",
        "                )\n",
        "                eval_onset_dts, eval_onset_keys = tuple(\n",
        "                    tensor.to(device) for tensor in eval_batch\n",
        "                )\n",
        "                eval_onset_key_logits, _, _ = model(eval_onset_dts, eval_onset_keys)\n",
        "                eval_loss_recons = F.cross_entropy(\n",
        "                    eval_onset_key_logits.view(-1, PIANO_NUM_KEYS),\n",
        "                    eval_onset_keys.view(-1),\n",
        "                    reduction=\"none\",\n",
        "                )\n",
        "                eval_metrics[\"loss_recons\"].extend(\n",
        "                    eval_loss_recons.cpu().numpy().tolist()\n",
        "                )\n",
        "\n",
        "            eval_loss_recons = np.mean(eval_metrics[\"loss_recons\"])\n",
        "            if eval_loss_recons < best_eval_loss_recons:\n",
        "                torch.save(model.state_dict(), pathlib.Path(run_dir, \"model.pt\"))\n",
        "                best_eval_loss_recons = eval_loss_recons\n",
        "\n",
        "        eval_metrics = {\"eval_loss_recons\": eval_loss_recons}\n",
        "        if USE_WANDB:\n",
        "            wandb.log(eval_metrics, step=step)\n",
        "        print(step, \"eval\", eval_metrics)\n",
        "\n",
        "        model.train()\n",
        "\n",
        "    # Create minibatch\n",
        "    batch = performances_to_batch(\n",
        "        random.sample(DATASET[\"train\"], CFG[\"batch_size\"]), device, train=True\n",
        "    )\n",
        "    onset_dts, onset_keys = tuple(tensor.to(device) for tensor in batch)\n",
        "\n",
        "    # Run model\n",
        "    optimizer.zero_grad()\n",
        "    onset_key_logits, onset_enc, _ = model(onset_dts, onset_keys)\n",
        "\n",
        "    # Compute losses and update params\n",
        "    loss_recons = F.cross_entropy(\n",
        "        onset_key_logits.view(-1, PIANO_NUM_KEYS), onset_keys.view(-1)\n",
        "    )\n",
        "    loss_margin = torch.square(\n",
        "        torch.maximum(torch.abs(onset_enc) - 1, torch.zeros_like(onset_enc))\n",
        "    ).mean()\n",
        "    loss_contour = torch.square(\n",
        "        torch.maximum(\n",
        "            1 - torch.diff(onset_keys, dim=1) * torch.diff(onset_enc, dim=1),\n",
        "            torch.zeros_like(onset_enc[:, 1:]),\n",
        "        )\n",
        "    ).mean()\n",
        "    loss = loss_recons\n",
        "    if CFG[\"loss_margin_multiplier\"] > 0:\n",
        "        loss += CFG[\"loss_margin_multiplier\"] * loss_margin\n",
        "    if CFG[\"loss_contour_multiplier\"] > 0:\n",
        "        loss += CFG[\"loss_contour_multiplier\"] * loss_contour\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    step += 1\n",
        "\n",
        "    if step % CFG[\"summarize_frequency\"] == 0:\n",
        "        metrics = {\n",
        "            \"loss_recons\": loss_recons.item(),\n",
        "            \"loss_margin\": loss_margin.item(),\n",
        "            \"loss_contour\": loss_contour.item(),\n",
        "            \"loss\": loss.item(),\n",
        "        }\n",
        "        if USE_WANDB:\n",
        "            wandb.log(metrics, step=step)\n",
        "        print(step, \"train\", metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj49WCPSAlyf",
        "cellView": "form"
      },
      "source": [
        "# @title **(Step 4)** Port trained decoder parameters to Tensorflow.js format\n",
        "\n",
        "!!pip install tensorflowjs\n",
        "\n",
        "from tensorflowjs.write_weights import write_weights\n",
        "\n",
        "# Load saved model dict\n",
        "d = torch.load(\"piano_genie/model.pt\", map_location=torch.device(\"cpu\"))\n",
        "d = {k: v.numpy() for k, v in d.items()}\n",
        "\n",
        "# Convert to tensorflow-js format\n",
        "pathlib.Path(\"piano_genie/dec_tfjs\").mkdir(exist_ok=True)\n",
        "write_weights(\n",
        "    [[{\"name\": k, \"data\": v} for k, v in d.items() if k.startswith(\"dec\")]],\n",
        "    \"piano_genie/dec_tfjs\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBuHu2Hohc5f",
        "cellView": "form"
      },
      "source": [
        "# @title **(Step 5)** Create test fixtures check correctness of JavaScript port\n",
        "\n",
        "# Restore model from saved checkpoint\n",
        "device = torch.device(\"cpu\")\n",
        "model = PianoGenieAutoencoder(CFG)\n",
        "model.load_state_dict(torch.load(\"piano_genie/model.pt\", map_location=device))\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "# Serialize a batch of inputs/outputs as JSON\n",
        "with torch.no_grad():\n",
        "    input_dts, ground_truth_keys = performances_to_batch(\n",
        "        [DATASET[\"validation\"][0]], device, train=False\n",
        "    )\n",
        "    output_logits, _, input_buttons = model(input_dts, ground_truth_keys)\n",
        "\n",
        "    input_dts = input_dts[0].cpu().numpy().tolist()\n",
        "    ground_truth_keys = ground_truth_keys[0].cpu().numpy().tolist()\n",
        "    input_keys = [PIANO_NUM_KEYS] + ground_truth_keys[:-1]\n",
        "    input_buttons = input_buttons[0].cpu().numpy().tolist()\n",
        "    output_logits = output_logits[0].cpu().numpy().tolist()\n",
        "\n",
        "    fixtures = {\n",
        "        n: eval(n)\n",
        "        for n in [\"input_dts\", \"input_keys\", \"input_buttons\", \"output_logits\"]\n",
        "    }\n",
        "    with open(pathlib.Path(\"piano_genie\", \"fixtures.json\"), \"w\") as f:\n",
        "        f.write(json.dumps(fixtures))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}