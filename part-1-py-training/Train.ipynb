{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Music Co-creation Tutorial Part 1 (Training).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "an946C19rSVJ"
      },
      "source": [
        "# @title **(1)** Parse MIDI piano performances into simple lists of notes\n",
        "\n",
        "USE_PRECACHED = True  # @param{type:\"boolean\"}\n",
        "PIANO_LOWEST_KEY_MIDI_PITCH = 21\n",
        "PIANO_NUM_KEYS = 88\n",
        "\n",
        "import gzip\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "def download_and_parse_maestro():\n",
        "    # Install pretty_midi\n",
        "    !!pip install pretty_midi\n",
        "    import pretty_midi\n",
        "\n",
        "    # Download MAESTRO dataset (Hawthorne+ 2018)\n",
        "    !!wget -nc https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip\n",
        "    !!unzip maestro-v2.0.0-midi.zip\n",
        "\n",
        "    # Parse MAESTRO dataset\n",
        "    dataset = defaultdict(list)\n",
        "    with open(\"maestro-v2.0.0/maestro-v2.0.0.json\", \"r\") as f:\n",
        "        for attrs in tqdm(json.load(f)):\n",
        "            split = attrs[\"split\"]\n",
        "            midi = pretty_midi.PrettyMIDI(\"maestro-v2.0.0/\" + attrs[\"midi_filename\"])\n",
        "            assert len(midi.instruments) == 1\n",
        "            notes = [\n",
        "                (\n",
        "                    float(n.start),\n",
        "                    float(n.end),\n",
        "                    int(n.pitch - PIANO_LOWEST_KEY_MIDI_PITCH),\n",
        "                    int(n.velocity),\n",
        "                )\n",
        "                for n in midi.instruments[0].notes\n",
        "            ]\n",
        "            notes = sorted(notes, key=lambda n: (n[0], n[2]))\n",
        "            assert all(\n",
        "                [\n",
        "                    all(\n",
        "                        [\n",
        "                            # Start times should be non-negative\n",
        "                            n[0] >= 0,\n",
        "                            # Note duration should be positive\n",
        "                            n[0] < n[1],\n",
        "                            # Key index should be in range of the piano\n",
        "                            0 <= n[2] and n[2] < PIANO_NUM_KEYS,\n",
        "                            # Velocity should be valid\n",
        "                            1 <= n[3] and n[3] < 128,\n",
        "                        ]\n",
        "                    )\n",
        "                    for n in notes\n",
        "                ]\n",
        "            )\n",
        "            dataset[split].append(notes)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "\n",
        "if USE_PRECACHED:\n",
        "    !!wget -nc https://github.com/chrisdonahue/music-cocreation-tutorial/raw/main/part-1-py-training/data/maestro-v2.0.0-simple.json.gz\n",
        "    with gzip.open(\"maestro-v2.0.0-simple.json.gz\", \"rb\") as f:\n",
        "        DATASET = json.load(f)\n",
        "else:\n",
        "    DATASET = download_and_parse_maestro()\n",
        "    with gzip.open(\"maestro-v2.0.0-simple.json.gz\", \"w\") as f:\n",
        "        f.write(json.dumps(DATASET).encode(\"utf-8\"))\n",
        "\n",
        "print([(s, len(DATASET[s])) for s in [\"train\", \"validation\", \"test\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_vnVHmw1VB5"
      },
      "source": [
        "# @title **(2)** Define Piano Genie modules\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class PianoGenieEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    PianoGenieEncoder maps each performance onset (D features) to a single scalar.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        rnn_dim=128,\n",
        "        rnn_num_layers=2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.rnn_dim = rnn_dim\n",
        "        self.rnn_num_layers = rnn_num_layers\n",
        "        self.input = nn.Linear(input_dim, rnn_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            rnn_dim,\n",
        "            rnn_dim,\n",
        "            rnn_num_layers,\n",
        "            bias=True,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "        )\n",
        "        self.output = nn.Linear(rnn_dim * 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        x = self.input(x)\n",
        "        h = (\n",
        "            torch.zeros(\n",
        "                2 * self.rnn_num_layers, batch_size, self.rnn_dim, device=x.device\n",
        "            ),\n",
        "            torch.zeros(\n",
        "                2 * self.rnn_num_layers, batch_size, self.rnn_dim, device=x.device\n",
        "            ),\n",
        "        )\n",
        "        x, h = self.lstm(x, h)\n",
        "        x = self.output(x)\n",
        "        return x[:, :, 0]\n",
        "\n",
        "\n",
        "class IntegerQuantizer(nn.Module):\n",
        "    \"\"\"\n",
        "    IntegerQuantizer independently quantizes scalar values to K values between [-1, 1].\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def real_to_discrete(self, x, eps=1e-6):\n",
        "        x = (x + 1) / 2\n",
        "        x = torch.clamp(x, 0, 1)\n",
        "        x *= self.vocab_size - 1\n",
        "        x = (torch.round(x) + eps).long()\n",
        "        return x\n",
        "\n",
        "    def discrete_to_real(self, x):\n",
        "        x = x.float()\n",
        "        x /= self.vocab_size - 1\n",
        "        x = (x * 2) - 1\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, output_discrete=False):\n",
        "        # Quantize and compute delta (used for straight-through estimator)\n",
        "        with torch.no_grad():\n",
        "            x_disc = self.real_to_discrete(x)\n",
        "            x_quant = self.discrete_to_real(x_disc)\n",
        "            x_quant_delta = x_quant - x\n",
        "\n",
        "        # Quantize w/ straight-through estimator\n",
        "        x = x + x_quant_delta\n",
        "\n",
        "        result = x\n",
        "        if output_discrete:\n",
        "            result = (x, x_disc)\n",
        "        return result\n",
        "\n",
        "\n",
        "class PianoGenieDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    PianoGenieDecoder maps buttons (quantized scalars) and onset features to key logits.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        output_dim,\n",
        "        rnn_dim=128,\n",
        "        rnn_num_layers=2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.rnn_dim = rnn_dim\n",
        "        self.rnn_num_layers = rnn_num_layers\n",
        "        self.input = nn.Linear(input_dim, rnn_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            rnn_dim,\n",
        "            rnn_dim,\n",
        "            rnn_num_layers,\n",
        "            bias=True,\n",
        "            batch_first=True,\n",
        "            bidirectional=False,\n",
        "        )\n",
        "        self.output = nn.Linear(rnn_dim, output_dim)\n",
        "\n",
        "    def init_hidden(self, batch_size, device=None):\n",
        "        h = torch.zeros(self.rnn_num_layers, batch_size, self.rnn_dim)\n",
        "        c = torch.zeros(self.rnn_num_layers, batch_size, self.rnn_dim)\n",
        "        if device is not None:\n",
        "            h = h.to(device)\n",
        "            c = c.to(device)\n",
        "        return (h, c)\n",
        "\n",
        "    def forward(self, x, h=None):\n",
        "        x = self.input(x)\n",
        "        x, h = self.lstm(x, h)\n",
        "        x = self.output(x)\n",
        "        return x, h\n",
        "\n",
        "\n",
        "class PianoGenieAutoencoder(nn.Module):\n",
        "    \"\"\"\n",
        "    PianoGenieAutoencoder composes encoder, quantizer, decoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.enc = None\n",
        "        if cfg[\"model_enc\"]:\n",
        "            self.enc = PianoGenieEncoder(\n",
        "                # Delta time + key one-hot\n",
        "                1 + PIANO_NUM_KEYS,\n",
        "                rnn_dim=cfg[\"model_rnn_dim\"],\n",
        "                rnn_num_layers=cfg[\"model_rnn_num_layers\"],\n",
        "            )\n",
        "            self.quant = IntegerQuantizer(cfg[\"num_buttons\"])\n",
        "        self.dec = PianoGenieDecoder(\n",
        "            # Delta time + key one-hot + <SOS> + Button\n",
        "            1 + PIANO_NUM_KEYS + 1 + int(cfg[\"model_enc\"]),\n",
        "            PIANO_NUM_KEYS,\n",
        "            rnn_dim=cfg[\"model_rnn_dim\"],\n",
        "            rnn_num_layers=cfg[\"model_rnn_num_layers\"],\n",
        "        )\n",
        "\n",
        "    def forward(self, onset_dts, onset_keys):\n",
        "        if self.enc is None:\n",
        "            enc = None\n",
        "            enc_quant_disc = None\n",
        "        else:\n",
        "            # Run encoder\n",
        "            enc = self.enc(\n",
        "                torch.cat(\n",
        "                    [\n",
        "                        # Feature: onset delta time\n",
        "                        onset_dts.unsqueeze(dim=2),\n",
        "                        # Feature: onset key index\n",
        "                        F.one_hot(onset_keys, PIANO_NUM_KEYS),\n",
        "                    ],\n",
        "                    dim=2,\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Quantize\n",
        "            enc_quant, enc_quant_disc = self.quant(enc, output_discrete=True)\n",
        "\n",
        "        # Run decoder\n",
        "        dec_inputs = [\n",
        "            # Feature: onset delta time\n",
        "            onset_dts.unsqueeze(dim=2),\n",
        "            # Feature: *last* onset key index (prepended with <SOS>)\n",
        "            F.one_hot(\n",
        "                torch.cat(\n",
        "                    [\n",
        "                        torch.full_like(onset_keys[:, :1], PIANO_NUM_KEYS),\n",
        "                        onset_keys[:, :-1],\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                ),\n",
        "                PIANO_NUM_KEYS + 1,\n",
        "            ),\n",
        "        ]\n",
        "        if self.enc is not None:\n",
        "            # Feature: button from encoder\n",
        "            dec_inputs.append(enc_quant.unsqueeze(dim=2))\n",
        "        onset_key_logits, _ = self.dec(torch.cat(dec_inputs, dim=2))\n",
        "\n",
        "        return onset_key_logits, enc, enc_quant_disc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_fReq-uCfoy"
      },
      "source": [
        "# @title **(3)** Train Piano Genie\n",
        "\n",
        "USE_WANDB = False  # @param{type:\"boolean\"}\n",
        "\n",
        "CFG = {\n",
        "    \"seed\": 0,\n",
        "    # Number of buttons in interface\n",
        "    \"num_buttons\": 8,\n",
        "    # Onset delta times will be clipped to this maximum\n",
        "    \"data_delta_time_max\": 1.0,\n",
        "    # Max time stretch for data augmentation (+- 5%)\n",
        "    \"data_augment_time_stretch_max\": 0.05,\n",
        "    # Max transposition for data augmentation (+- tritone)\n",
        "    \"data_augment_transpose_max\": 6,\n",
        "    # Enables encoder\n",
        "    \"model_enc\": True,\n",
        "    # RNN dimensionality\n",
        "    \"model_rnn_dim\": 128,\n",
        "    # RNN num layers\n",
        "    \"model_rnn_num_layers\": 2,\n",
        "    # Training hyperparameters\n",
        "    \"batch_size\": 32,\n",
        "    \"seq_len\": 128,\n",
        "    \"lr\": 3e-4,\n",
        "    \"loss_margin_multiplier\": 1.0,\n",
        "    \"loss_contour_multiplier\": 1.0,\n",
        "    \"summarize_frequency\": 128,\n",
        "    \"eval_frequency\": 128,\n",
        "}\n",
        "\n",
        "import pathlib\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "if USE_WANDB:\n",
        "    try:\n",
        "        import wandb\n",
        "    except ModuleNotFoundError:\n",
        "        !!pip install wandb\n",
        "        import wandb\n",
        "\n",
        "# Init\n",
        "run_dir = pathlib.Path(\"piano_genie\")\n",
        "run_dir.mkdir(exist_ok=True)\n",
        "with open(pathlib.Path(run_dir, \"cfg.json\"), \"w\") as f:\n",
        "    f.write(json.dumps(CFG, indent=2))\n",
        "if USE_WANDB:\n",
        "    wandb.init(project=\"piano-genie\", name=\"tutorial\", config=CFG, reinit=True)\n",
        "\n",
        "# Set seed\n",
        "if CFG[\"seed\"] is not None:\n",
        "    random.seed(CFG[\"seed\"])\n",
        "    np.random.seed(CFG[\"seed\"])\n",
        "    torch.manual_seed(CFG[\"seed\"])\n",
        "    torch.cuda.manual_seed_all(CFG[\"seed\"])\n",
        "\n",
        "# Create models\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = PianoGenieAutoencoder(CFG)\n",
        "model.train()\n",
        "model.to(device)\n",
        "print(\"-\" * 80)\n",
        "for n, p in model.named_parameters():\n",
        "    print(f\"{n}, {p.shape}\")\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=CFG[\"lr\"])\n",
        "\n",
        "# Subsamples performances to create a minibatch\n",
        "def performances_to_batch(performances, device, train=True):\n",
        "    batch_onset_dt = []\n",
        "    batch_onset_keys = []\n",
        "    for p in performances:\n",
        "        # Subsample seq_len notes from performance\n",
        "        assert len(p) >= CFG[\"seq_len\"]\n",
        "        if train:\n",
        "            subsample_offset = random.randrange(0, len(p) - CFG[\"seq_len\"])\n",
        "        else:\n",
        "            subsample_offset = 0\n",
        "        subsample = p[subsample_offset : subsample_offset + CFG[\"seq_len\"]]\n",
        "        assert len(subsample) == CFG[\"seq_len\"]\n",
        "\n",
        "        # Data augmentation\n",
        "        if train:\n",
        "            stretch_factor = (\n",
        "                1\n",
        "                + (random.random() * CFG[\"data_augment_time_stretch_max\"] * 2)\n",
        "                - CFG[\"data_augment_time_stretch_max\"]\n",
        "            )\n",
        "            transposition_factor = random.randint(\n",
        "                -CFG[\"data_augment_transpose_max\"], CFG[\"data_augment_transpose_max\"]\n",
        "            )\n",
        "            subsample = [\n",
        "                (\n",
        "                    n[0] * stretch_factor,\n",
        "                    n[1] * stretch_factor,\n",
        "                    max(0, min(n[2] + transposition_factor, PIANO_NUM_KEYS - 1)),\n",
        "                    n[3],\n",
        "                )\n",
        "                for n in subsample\n",
        "            ]\n",
        "\n",
        "        # Compute onset features\n",
        "        onset_dt = np.diff([n[0] for n in subsample])\n",
        "        onset_dt = np.concatenate([[1e8], onset_dt])\n",
        "        onset_dt = np.clip(onset_dt, 0, CFG[\"data_delta_time_max\"])\n",
        "        batch_onset_dt.append(onset_dt)\n",
        "\n",
        "        # Compute onset keys\n",
        "        batch_onset_keys.append([n[2] for n in subsample])\n",
        "\n",
        "    return (\n",
        "        torch.tensor(np.array(batch_onset_dt)).float(),\n",
        "        torch.tensor(np.array(batch_onset_keys)).long(),\n",
        "    )\n",
        "\n",
        "\n",
        "# Train\n",
        "step = 0\n",
        "best_eval_loss_recons = float(\"inf\")\n",
        "while True:\n",
        "    if step % CFG[\"eval_frequency\"] == 0:\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            eval_metrics = defaultdict(list)\n",
        "            for i in range(0, len(DATASET[\"validation\"]), CFG[\"batch_size\"]):\n",
        "                eval_batch = performances_to_batch(\n",
        "                    DATASET[\"validation\"][i : i + CFG[\"batch_size\"]],\n",
        "                    device,\n",
        "                    train=False,\n",
        "                )\n",
        "                eval_onset_dts, eval_onset_keys = tuple(\n",
        "                    tensor.to(device) for tensor in eval_batch\n",
        "                )\n",
        "                eval_onset_key_logits, _, _ = model(eval_onset_dts, eval_onset_keys)\n",
        "                eval_loss_recons = F.cross_entropy(\n",
        "                    eval_onset_key_logits.view(-1, PIANO_NUM_KEYS),\n",
        "                    eval_onset_keys.view(-1),\n",
        "                    reduction=\"none\",\n",
        "                )\n",
        "                eval_metrics[\"loss_recons\"].extend(\n",
        "                    eval_loss_recons.cpu().numpy().tolist()\n",
        "                )\n",
        "\n",
        "            eval_loss_recons = np.mean(eval_metrics[\"loss_recons\"])\n",
        "            if eval_loss_recons < best_eval_loss_recons:\n",
        "                torch.save(model.state_dict(), pathlib.Path(run_dir, \"model.pt\"))\n",
        "                best_eval_loss_recons = eval_loss_recons\n",
        "\n",
        "        eval_metrics = {\"eval_loss_recons\": eval_loss_recons}\n",
        "        if USE_WANDB:\n",
        "            wandb.log(eval_metrics, step=step)\n",
        "        print(step, \"eval\", eval_metrics)\n",
        "\n",
        "        model.train()\n",
        "\n",
        "    # Create minibatch\n",
        "    batch = performances_to_batch(\n",
        "        random.sample(DATASET[\"train\"], CFG[\"batch_size\"]), device, train=True\n",
        "    )\n",
        "    onset_dts, onset_keys = tuple(tensor.to(device) for tensor in batch)\n",
        "\n",
        "    # Run model\n",
        "    optimizer.zero_grad()\n",
        "    onset_key_logits, onset_enc, _ = model(onset_dts, onset_keys)\n",
        "\n",
        "    # Compute losses and update params\n",
        "    loss_recons = F.cross_entropy(\n",
        "        onset_key_logits.view(-1, PIANO_NUM_KEYS), onset_keys.view(-1)\n",
        "    )\n",
        "    loss_margin = torch.square(\n",
        "        torch.maximum(torch.abs(onset_enc) - 1, torch.zeros_like(onset_enc))\n",
        "    ).mean()\n",
        "    loss_contour = torch.square(\n",
        "        torch.maximum(\n",
        "            1 - torch.diff(onset_keys, dim=1) * torch.diff(onset_enc, dim=1),\n",
        "            torch.zeros_like(onset_enc[:, 1:]),\n",
        "        )\n",
        "    ).mean()\n",
        "    loss = loss_recons\n",
        "    if CFG[\"loss_margin_multiplier\"] > 0:\n",
        "        loss += CFG[\"loss_margin_multiplier\"] * loss_margin\n",
        "    if CFG[\"loss_contour_multiplier\"] > 0:\n",
        "        loss += CFG[\"loss_contour_multiplier\"] * loss_contour\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    step += 1\n",
        "\n",
        "    if step % CFG[\"summarize_frequency\"] == 0:\n",
        "        metrics = {\n",
        "            \"loss_recons\": loss_recons.item(),\n",
        "            \"loss_margin\": loss_margin.item(),\n",
        "            \"loss_contour\": loss_contour.item(),\n",
        "            \"loss\": loss.item(),\n",
        "        }\n",
        "        if USE_WANDB:\n",
        "            wandb.log(metrics, step=step)\n",
        "        print(step, \"train\", metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj49WCPSAlyf"
      },
      "source": [
        "# @title **(3)** Port trained decoder parameters to Tensorflow.js format\n",
        "\n",
        "!!pip install tensorflowjs\n",
        "\n",
        "from tensorflowjs.write_weights import write_weights\n",
        "\n",
        "# Load saved model dict\n",
        "d = torch.load(\"piano_genie/model.pt\", map_location=torch.device(\"cpu\"))\n",
        "d = {k: v.numpy() for k, v in d.items()}\n",
        "\n",
        "# Convert to tensorflow-js format\n",
        "pathlib.Path(\"piano_genie/dec_tfjs\").mkdir(exist_ok=True)\n",
        "write_weights(\n",
        "    [[{\"name\": k, \"data\": v} for k, v in d.items() if k.startswith(\"dec\")]],\n",
        "    \"piano_genie/dec_tfjs\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBuHu2Hohc5f"
      },
      "source": [
        "# @title **(4)** Create test fixtures check correctness of JavaScript port\n",
        "\n",
        "# Restore model from saved checkpoint\n",
        "device = torch.device(\"cpu\")\n",
        "model = PianoGenieAutoencoder(CFG)\n",
        "model.load_state_dict(torch.load(\"piano_genie/model.pt\", map_location=device))\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "# Serialize a batch of inputs/outputs as JSON\n",
        "with torch.no_grad():\n",
        "    input_dts, ground_truth_keys = performances_to_batch(\n",
        "        [DATASET[\"validation\"][0]], device, train=False\n",
        "    )\n",
        "    output_logits, _, input_buttons = model(input_dts, ground_truth_keys)\n",
        "\n",
        "    input_dts = input_dts[0].cpu().numpy().tolist()\n",
        "    ground_truth_keys = ground_truth_keys[0].cpu().numpy().tolist()\n",
        "    input_keys = [PIANO_NUM_KEYS] + ground_truth_keys[:-1]\n",
        "    input_buttons = input_buttons[0].cpu().numpy().tolist()\n",
        "    output_logits = output_logits[0].cpu().numpy().tolist()\n",
        "\n",
        "    fixtures = {\n",
        "        n: eval(n)\n",
        "        for n in [\"input_dts\", \"input_keys\", \"input_buttons\", \"output_logits\"]\n",
        "    }\n",
        "    with open(pathlib.Path(\"piano_genie\", \"fixtures.json\"), \"w\") as f:\n",
        "        f.write(json.dumps(fixtures))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}